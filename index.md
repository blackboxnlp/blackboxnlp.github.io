# BlackboxNLP 2023

There will be a sixth edition of BlackboxNLP! 
It will be co-located with EMNLP 2023.

## News

- BlackboxNLP will be back in 2023 co-located with EMNLP 2023!
- We started a YouTube channel: [https://www.youtube.com/@blackboxnlp](https://www.youtube.com/@blackboxnlp). Subscribe to be informed of all upcoming content. You can already watch the BlackboxNLP 2022 keynotes. 

## Important dates

TBD

All deadlines are 11:59pm UTC-12 ("anywhere on earth").

## Workshop description

Many recent performance improvements in NLP have come at the cost of understanding of the systems. How do we assess what representations and computations models learn? How do we formalize desirable properties of interpretable models, and measure the extent to which existing models achieve them? How can we build models that better encode these properties? What can new or existing tools tell us about systemsâ€™ inductive biases?

The goal of this workshop is to bring together researchers focused on interpreting and explaining NLP models by taking inspiration from machine learning, psychology, linguistics, and neuroscience. We hope the workshop will serve as an interdisciplinary meetup that allows for cross-collaboration.

The topics of the workshop include, but are not limited to:
- Explanation methods such as saliency, attribution, free-text explanations, or explanations with structured properties
- Probing methods for testing whether models have acquired or represent certain linguistic properties
- Applying analysis techniques from other disciplines (e.g., neuroscience or computer vision)
- Examining model performance on simplified or formal languages
- More interpretable model architectures
- Open-source tools for analysis, visualization, or explanation;
- Evaluation of explanation methods
- Opinion pieces about the state of explainable NLP

Feel free to reach out to the organizers at the email below if you are not sure whether a specific topic is well-suited for submission.

## Call for Papers

TBD

### Submission Types

TBD

### Dual Submissions and Preprints
Dual submissions are **not** allowed for the archival track. Papers posted to preprint servers such as arxiv can be submitted without any restrictions on when they were posted.

### Camera-ready information
Authors of accepted archival papers should upload the final version of their paper to the submission system by the camera-ready deadline. Authors may use **one extra page** to address reviewer comments, for a total of nine pages + references. Broader Impacts/Ethics and Limitations sections are optional and can be included on a 10th page.

## Contact
Please contact the organizers at blackboxnlp@googlegroups.com for any questions.

## Previous workshops

- [BlackboxNLP 2018](https://blackboxnlp.github.io/2018/) (at EMNLP 2018)
- [BlackboxNLP 2019](https://blackboxnlp.github.io/2019/) (at ACL 2019)
- [BlackboxNLP 2020](https://blackboxnlp.github.io/2020/) (at EMNLP 2020)
- [BlackboxNLP 2021](https://blackboxnlp.github.io/2021/) (at EMNLP 2021)
- [BlackboxNLP 2022](https://blackboxnlp.github.io/2022/) (at EMNLP 2022)

## Sponsors

TBD

## Organizers

You can reach the organizers by e-mail to <a href="mailto:blackboxnlp@googlegroups.com">blackboxnlp@googlegroups.com</a>.

### Yonatan Belinkov
Yonatan Belinkov is an assistant professor at the Technion. 
He has previously been a Postdoctoral Fellow at Harvard and MIT.
His recent research focuses on interpretability and robustness of neural network models of language. 
His research has been published at leading NLP and ML venues. 
His PhD dissertation at MIT analyzed internal language representations in deep learning models.
He has been awarded the Harvard Mind, Brain, and Behavior Postdoctoral Fellowship and the Azrieli Early Career Faculty Fellowship.
He co-organised BlackboxNLP in 2019, 2020, and 2021, as well as the 1st and 2nd machine translation robustness tasks at WMT.


## Anti-Harassment Policy
BlackboxNLP 2023 adheres to the [ACL Anti-Harassment Policy](https://www.aclweb.org/adminwiki/sphp?title=Anti-Harassment_Policy).
