# BlackboxNLP 2023
The sixth edition of BlackboxNLP will be co-located with EMNLP, in Singapore on December 7 2023.

## News
- We welcome EMNLP Findings papers with a focus on model interpretability for presentation at the BlackboxNLP workshop. Please contact us via [blackboxnlp@googlegroups.com](blackboxnlp@googlegroups.com) by the 25th of October, noon ET.
- We started a YouTube channel: [https://www.youtube.com/@blackboxnlp](https://www.youtube.com/@blackboxnlp). Subscribe to be informed of all upcoming content. You can already watch the BlackboxNLP 2022 keynotes.
- Find us on Twitter/X here: [https://twitter.com/blackboxnlp](https://twitter.com/blackboxnlp)

## Invited Speakers
### Zhijing Jin
<p><img src="img/Zhijing.jpg" width="150px" height="150px"></p>

> Causal NLP: A Path towards Opening the Black Box of NLP

Zhijing Jin is a Ph.D. at Max Planck Institute & ETH. Her research focuses on socially responsible NLP via causal and moral principles. Specifically, she works on expanding the impact of NLP by promoting NLP for social good, and developing CausalNLP to improve robustness, fairness, and interpretability of NLP models, as well as analyze the causes of social problems.

### Asli Celikyilmaz
<p><img src="img/Asli.png" width="150px"></p>

> Charting New Pathways: Next-Gen LLM Reasoners

Asli Celikyilmaz is a Research Science Manager at Fundamentals AI Research (FAIR) at Meta. She is also an Affiliate Associate Member at the University of Washington. She has received Ph.D. Degree in Systems Engineering from University of Toronto, Canada, and later continued her Postdoc study at Computer Science Department of the University of California, Berkeley. Her research interests are mainly in language generation with long-term coherence, LLM reasoning and alignment, language grounding with vision, and building intelligent agents for human-computer interaction.


## Important dates

- ~~**September 1, 2023** - Paper submission deadline (via Softconf: [https://www.softconf.com/emnlp2023/blackboxnlp2023/](https://www.softconf.com/emnlp2023/blackboxnlp2023/)).~~
- ~~**October 6, 2023** - Notification of acceptance.~~
- ~~**October 18, 2023** - Camera ready deadline.~~
- **December 7, 2023** - Workshop date.

All deadlines are 11:59pm UTC-12 ("anywhere on earth").

## Workshop description

Many recent performance improvements in NLP have come at the cost of understanding of the systems. How do we assess what representations and computations models learn? How do we formalize desirable properties of interpretable models, and measure the extent to which existing models achieve them? How can we build models that better encode these properties? What can new or existing tools tell us about these systems’ inductive biases?

The goal of this workshop is to bring together researchers focused on interpreting and explaining NLP models by taking inspiration from fields such as machine learning, psychology, linguistics, and neuroscience. We hope the workshop will serve as an interdisciplinary meetup that allows for cross-collaboration.

The topics of the workshop include, but are not limited to:
- Explanation methods such as saliency, attribution, free-text explanations, or explanations with structured properties.
- Mechanistic interpretability, reverse engineering approaches to understanding particular properties of neural models.
- Probing methods for testing whether models have acquired or represent certain linguistic properties.
- Applying analysis techniques from other disciplines (e.g., neuroscience or computer vision).
- Examining model performance on simplified or formal languages.
- Proposing modifications to neural architectures that increase their interpretability.
- Open-source tools for analysis, visualization, or explanation.
- Evaluation of explanation methods: how do we know the explanation is faithful to the model?
- Opinion pieces about the state of explainable NLP.

Feel free to reach out to the organizers at the email below if you are not sure whether a specific topic is well-suited for submission.

## Call for Papers
We will accept submissions through Softconf at: [https://www.softconf.com/emnlp2023/blackboxnlp2023/](https://www.softconf.com/emnlp2023/blackboxnlp2023/). All submissions should use the EMNLP 2023 [template](https://www.overleaf.com/latex/templates/instructions-for-emnlp-2023-proceedings/scyjxmtnrskr) and formatting requirements specified by [ACL](https://acl-org.github.io/ACLPUB/formatting.html). Archival paper must be fully anonymized. Submissions of both types can be made through [Softconf](https://www.softconf.com/emnlp2023/blackboxnlp2023/).


### Submission Types
- **Archival papers** of up to 8 pages + references. These are papers reporting on completed, original and unpublished research. An optional appendix may appear after the references in the same pdf file. Accepted papers are expected to be presented at the workshop and will be published in the workshop proceedings of the ACL Anthology, meaning they cannot be published elsewhere. They should report on obtained results rather than intended work. Broader Impacts/Ethics and Limitations sections are optional and can be included on a 9th page.
- **Non-archival extended abstracts** of 2 pages + references. These may report on work in progress or may be cross-submissions of work that has already appeared (or is scheduled to appear) in another venue in 2022-2023. Abstract titles will be posted on the workshop website but will not be included in the proceedings. The selection will not be based on a double-blind review and thus submissions of this type need not be anonymized. 

Accepted submissions for both tracks will be presented at the workshop: most as posters, some as oral presentations (determined by the program committee).


### Dual Submissions and Preprints
Dual submissions **are** allowed for the archival track, but please check the dual submissions policy for the other venue that you are dual-submitting to. Papers posted to preprint servers such as arXiv can be submitted without any restrictions on when they were posted.

### Camera-ready information
Authors of accepted archival papers should upload the final version of their paper to the submission system by the camera-ready deadline. Authors may use **one extra page** to address reviewer comments, for a total of nine pages + references. Broader Impacts/Ethics and Limitations sections are optional and can be included on a 10th page.

## Contact
Please contact the organizers at <a href="mailto:blackboxnlp@googlegroups.com">blackboxnlp@googlegroups.com</a> for any questions.

## Previous workshops

- [BlackboxNLP 2018](https://blackboxnlp.github.io/2018/) (at EMNLP 2018)
- [BlackboxNLP 2019](https://blackboxnlp.github.io/2019/) (at ACL 2019)
- [BlackboxNLP 2020](https://blackboxnlp.github.io/2020/) (at EMNLP 2020)
- [BlackboxNLP 2021](https://blackboxnlp.github.io/2021/) (at EMNLP 2021)
- [BlackboxNLP 2022](https://blackboxnlp.github.io/2022/) (at EMNLP 2022)

## Sponsors

* Google
* Apple

## Organizers

You can reach the organizers by e-mail to <a href="mailto:blackboxnlp@googlegroups.com">blackboxnlp@googlegroups.com</a>.

### Yonatan Belinkov
Yonatan Belinkov is an assistant professor at the Technion. 
He has previously been a Postdoctoral Fellow at Harvard and MIT.
His recent research focuses on interpretability and robustness of neural network models of language. 
His research has been published at leading NLP and ML venues. 
His PhD dissertation at MIT analyzed internal language representations in deep learning models.
He has been awarded the Harvard Mind, Brain, and Behavior Postdoctoral Fellowship and the Azrieli Early Career Faculty Fellowship.
He co-organised BlackboxNLP in 2019, 2020, and 2021, as well as the 1st and 2nd machine translation robustness tasks at WMT.

### Najoung Kim
Najoung Kim is an Assistant Professor at the Department of Linguistics at Boston University. She is currently visting Google Research part-time. She is interested in studying meaning in both human and machine learners, especially ways in which they generalize to novel inputs and ways in which they treat implicit meaning. Her research has been published in various NLP venues including ACL and EMNLP. She was a co-organizer of the Inverse Scaling Competition, and a senior area chair for ACL 2023.

### Sophie Hao
Sophie Hao is a Faculty Fellow at the Center for Data Science at New York University. She recently completed her PhD in Linguistics and Computer Science at Yale University, where her dissertation work focused on applications of feature attribution methods to NLP. More generally, Sophie is interested in theories of interpretation and explanation and how such theories can guide our usage and evaluation of analysis methods for black-box models. She is a frequent contributor to BlackboxNLP, having presented at its first three editions.

### Arya McCarthy
Arya McCarthy is a Research Scientist at [Scaled Cognition][sc]. He previously was a Ph.D. student in the Center for Language and Speech Processing at the Johns Hopkins University. He is supported by an Amazon Fellowship, a Frederick Jelinek Fellowship, and the International Olympic Committee. He investigates pan-lingual weak supervision for building core NLP tools and datasets in new languages; recent work has investigated limitations of large language models on simple transduction tasks. His research has been published in ACL, EMNLP, CoNLL, COLING, ICLR, and ICASSP.

### Jaap Jumelet
Jaap Jumelet is a PhD candidate at the Institute for Logic, Language and Computation at the University of Amsterdam. His research focuses on gaining an understanding of how neural models are able to build up hierarchical representations of their input, by leveraging hypotheses from (psycho-)linguistics. His research has been published at leading NLP venues, including TACL, ACL, and CoNLL. His first ever paper was presented at the first BlackboxNLP workshop in 2018, and he has since presented work at each subsequent edition of the workshop. 

### Hosein Mohebbi
Hosein Mohebbi is a PhD candidate at the Department of Cognitive Science and Artificial Intelligence at Tilburg University. He is part of the InDeep consortium, working on analyzing and interpreting deep neural language and speech models. During his Master’s (2019-2021), he mainly focused on interpretability and accelerating inference of pre-trained language models. His research has been published in NLP venues such as ACL, EACL, EMNLP and BlackboxNLP.


## Anti-Harassment Policy
BlackboxNLP 2023 adheres to the [ACL Anti-Harassment Policy](https://www.aclweb.org/adminwiki/sphp?title=Anti-Harassment_Policy).


[sc]: https://scaledcognition.com
